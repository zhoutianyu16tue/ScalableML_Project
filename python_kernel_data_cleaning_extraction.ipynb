{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import hops.hdfs as hdfs\n",
    "import tarfile\n",
    "import tables\n",
    "import os\n",
    "import string\n",
    "fs_handle = hdfs.get_fs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/srv/hops/staging/private_dirs/a584dcd0f2d97115248b094f8bb4cca326af95d74ddbbef145147b1e0ec89349\n"
     ]
    }
   ],
   "source": [
    "print(os.environ['PDIR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_segments_timbre(h5,songidx=0):\n",
    "    \n",
    "    if h5.root.analysis.songs.nrows == songidx + 1:\n",
    "        return h5.root.analysis.segments_timbre[h5.root.analysis.songs.cols.idx_segments_timbre[songidx]:,:]\n",
    "    return h5.root.analysis.segments_timbre[h5.root.analysis.songs.cols.idx_segments_timbre[songidx]:\n",
    "                                            h5.root.analysis.songs.cols.idx_segments_timbre[songidx+1],:]\n",
    "\n",
    "def get_year(h5,songidx=0):\n",
    "\n",
    "    return h5.root.musicbrainz.songs.cols.year[songidx]\n",
    "\n",
    "def get_artist_id(h5,songidx=0):\n",
    "\n",
    "    return h5.root.metadata.songs.cols.artist_id[songidx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def read_target_track_id_list():\n",
    "    # Read the track ids with top 100 tags\n",
    "    with fs_handle.open_file(hdfs.project_path() + 'Resources/track_id_with_top_100_tag.txt', 'r') as fd:\n",
    "        data = fd.read()\n",
    "    \n",
    "    return data.split('\\n')[:-1]\n",
    "\n",
    "def put_target_track_into_buckets(target_track_id_list):\n",
    "    \n",
    "    buckets = {}\n",
    "    \n",
    "    for upper_letter in string.ascii_uppercase:\n",
    "         buckets[upper_letter] = filter(lambda track_id: track_id[2] == upper_letter, target_track_id_list)\n",
    "            \n",
    "    return buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "369047\n",
      "26\n"
     ]
    }
   ],
   "source": [
    "target_track_id_list = read_target_track_id_list()\n",
    "print(len(target_track_id_list))\n",
    "\n",
    "target_tracks_buckets = put_target_track_into_buckets(target_track_id_list)\n",
    "print(len(target_tracks_buckets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method copy in module pydoop.hdfs.fs:\n",
      "\n",
      "copy(self, from_path, to_hdfs, to_path) method of pydoop.hdfs.fs.hdfs instance\n",
      "    Copy file from one filesystem to another.\n",
      "    \n",
      "    :type from_path: str\n",
      "    :param from_path: the path of the source file\n",
      "    :type to_hdfs: :class:`hdfs`\n",
      "    :param to_hdfs: destination filesystem\n",
      "    :type to_path: str\n",
      "    :param to_path: the path of the destination file\n",
      "    :raises: :exc:`~exceptions.IOError`\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14337"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "help(fs_handle.copy)\n",
    "len(target_tracks_buckets['C'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def move_h5_file_to_local_path(tar_file_handle, track_id, feature_file_handle):\n",
    "    \n",
    "    extract_file_name = track_id[2] + '/' + \\\n",
    "                        track_id[3] + '/' + \\\n",
    "                        track_id[4] + '/' + \\\n",
    "                        track_id + '.h5'\n",
    "    \n",
    "#     print('Extracting', extract_file_name)\n",
    "    f_obj = tar_file_handle.extractfile(extract_file_name)\n",
    "    \n",
    "#     print('Writing %s into hdfs path' % (track_id))\n",
    "    extract_h5_file_path = hdfs.project_path() + \"Resources/extract_h5_tmp.h5\"\n",
    "    with fs_handle.open_file(extract_h5_file_path, flags='w') as fd:\n",
    "        fd.write(f_obj.read())\n",
    "    \n",
    "    f_obj.close()\n",
    "    \n",
    "#     print('Moving %s into local path' % (track_id))\n",
    "#     copy the h5 file from hdfs filesystem to local filesystem\n",
    "    h5_lcoal_path = os.environ['PDIR']\n",
    "    hdfs.hdfs.get(extract_h5_file_path, h5_lcoal_path)\n",
    "    \n",
    "#     print('Reading %s from local path' % (track_id))\n",
    "#     Extract some info from the h5 file\n",
    "    with tables.open_file(os.environ['PDIR'] + '/extract_h5_tmp.h5', mode=\"r\") as h5_file_handle:\n",
    "#         print('segments_timbre shape:', get_segments_timbre(h5_file_handle).shape)\n",
    "        \n",
    "        mfcc_vec = get_segments_timbre(h5_file_handle).reshape(-1).tolist()\n",
    "        mfcc_vec.insert(0, track_id)\n",
    "        \n",
    "        to_str_mfcc_vec = list(map(lambda v: str(v), mfcc_vec))\n",
    "        feature_file_handle.write(','.join(to_str_mfcc_vec) + '\\n')\n",
    "#         print()\n",
    "    \n",
    "#     print('Removing extract_h5_tmp.h')\n",
    "#     remove the tmp h5 file to ensure upcoming file move\n",
    "    os.system('rm %s/%s' % (os.environ['PDIR'], 'extract_h5_tmp.h5'))\n",
    "    \n",
    "#     print('Leaving move_h5_file_to_local_path')\n",
    "    \n",
    "def extract_info_from_tgz(bucket):\n",
    "    \n",
    "    file_path = 'hdfs:///Projects/labs/million_song/huge_dataset/%s.tar.gz' % (bucket)\n",
    "    \n",
    "    print('Moving %s.tar.gz into local path' % (bucket))\n",
    "    hdfs.hdfs.get(file_path, os.environ['PDIR'])\n",
    "    print('Finished moving %s.tar.gz into local path' % (bucket))\n",
    "#     fd = fs_handle.open_file(file_path, 'r')\n",
    "\n",
    "    print('Open %s.tar.gz file' % (bucket))\n",
    "    tar_file_handle = tarfile.open(os.environ['PDIR'] + '/%s.tar.gz' % (bucket), mode=\"r\")\n",
    "    \n",
    "#     print('extracting h5 files')\n",
    "#     h5_file_names = filter(lambda f: f.endswith('.h5'), tar_file_handle.getnames())\n",
    "    \n",
    "    feature_file_handle = fs_handle.open_file(hdfs.project_path() + \"Resources/audio_feactures/%s/mfcc_vectors.txt\" % (bucket), \n",
    "                             flags='w')\n",
    "\n",
    "    for h5_file_name in target_tracks_buckets[bucket]:\n",
    "        move_h5_file_to_local_path(tar_file_handle, h5_file_name, feature_file_handle)\n",
    "    \n",
    "    feature_file_handle.close()\n",
    "    tar_file_handle.close()\n",
    "    print('removing %s' % (os.environ['PDIR'] + '/%s.tar.gz' % (bucket)))\n",
    "    os.system('rm %s' % (os.environ['PDIR'] + '/%s.tar.gz' % (bucket)))\n",
    "    \n",
    "    print('Leaving extract_info_from_tgz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving A.tar.gz into local path\n",
      "Finished moving A.tar.gz into local path\n",
      "Open A.tar.gz file\n"
     ]
    }
   ],
   "source": [
    "extract_info_from_tgz('A')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on A\n",
      "extracting h5 files\n",
      "removing /srv/hops/staging/private_dirs/a584dcd0f2d97115248b094f8bb4cca326af95d74ddbbef145147b1e0ec89349/A.tar.gz\n",
      "('h5_file_cnt', 39100)\n",
      "Working on C\n",
      "extracting h5 files\n",
      "removing /srv/hops/staging/private_dirs/a584dcd0f2d97115248b094f8bb4cca326af95d74ddbbef145147b1e0ec89349/C.tar.gz\n",
      "('h5_file_cnt', 38611)\n",
      "Working on B\n",
      "extracting h5 files\n",
      "removing /srv/hops/staging/private_dirs/a584dcd0f2d97115248b094f8bb4cca326af95d74ddbbef145147b1e0ec89349/B.tar.gz\n",
      "('h5_file_cnt', 38265)\n",
      "Working on E\n",
      "extracting h5 files\n",
      "removing /srv/hops/staging/private_dirs/a584dcd0f2d97115248b094f8bb4cca326af95d74ddbbef145147b1e0ec89349/E.tar.gz\n",
      "('h5_file_cnt', 38466)\n",
      "Working on D\n",
      "extracting h5 files\n",
      "removing /srv/hops/staging/private_dirs/a584dcd0f2d97115248b094f8bb4cca326af95d74ddbbef145147b1e0ec89349/D.tar.gz\n",
      "('h5_file_cnt', 38825)\n",
      "Working on G\n",
      "extracting h5 files\n",
      "removing /srv/hops/staging/private_dirs/a584dcd0f2d97115248b094f8bb4cca326af95d74ddbbef145147b1e0ec89349/G.tar.gz\n",
      "('h5_file_cnt', 38156)\n",
      "Working on F\n",
      "extracting h5 files\n",
      "removing /srv/hops/staging/private_dirs/a584dcd0f2d97115248b094f8bb4cca326af95d74ddbbef145147b1e0ec89349/F.tar.gz\n",
      "('h5_file_cnt', 38919)\n",
      "Working on I\n"
     ]
    }
   ],
   "source": [
    "total_h5_file_cnt = 0\n",
    "\n",
    "for bucket, tracks in target_tracks_buckets.items():\n",
    "    print('Working on %s' % bucket)\n",
    "    h5_file_cnt = extract_info_from_tgz(bucket)\n",
    "    print('h5_file_cnt', h5_file_cnt)\n",
    "    \n",
    "    total_h5_file_cnt += h5_file_cnt\n",
    "    \n",
    "print('total_h5_file_cnt', total_h5_file_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "logfile = hdfs.project_path() + \"Resources/A/file.txt\"\n",
    "fd = fs_handle.open_file(logfile, flags='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fd.write('Hello HopsWorksjhjashdas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fd.write('Hello HopsWorksasdas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fd.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
